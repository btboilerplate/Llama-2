# ğŸ¦™ Llama-2 Fine-Tuning with QLoRA (Guanaco Instruction Tuning)

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Transformers](https://img.shields.io/badge/HuggingFace-Transformers-yellow.svg)
![PEFT](https://img.shields.io/badge/PEFT-QLoRA-orange.svg)

This project demonstrates instruction fine-tuning of **LLaMA-2** using **QLoRA**, including dataset preparation, efficient training, and iterative inference debugging to achieve clean instruction-style responses.

---

## ğŸ“Œ Project Overview

The objective was to fine-tune LLaMA-2 for Guanaco-style instruction following while working under limited GPU resources.

Key components:

- Dataset formatting for LLaMA-2 chat template
- QLoRA parameter-efficient fine-tuning
- Training monitoring
- Iterative inference improvements

---

## ğŸ“ Project Structure

- llama2.ipynb â€” Main fine-tuning pipeline  
- dataset.ipynb â€” Dataset formatter  
- requirements.txt â€” Project dependencies  
- assets/ â€” Training and inference visual outputs  
- llama_train_dataset/ â€” Processed dataset  
- results/ â€” Training checkpoints  
- README.md â€” Project documentation  

---

## ğŸ“¦ Requirements

Install dependencies using:
text
```
pip install -r requirements.txt
```
---

## ğŸ“Š Dataset Strategy

### Attempted Dataset (Custom Formatting)

Dataset formatter created for:

**timdettmers/openassistant-guanaco**

Full fine-tuning skipped due to estimated training time:

â± **14â€“24 hours depending on GPU**

---

### Final Dataset Used

**mlabonne/guanaco-llama2-1k**

â± Fine-tuning completed in:

**2:33:04 (1 epoch)**

---

## ğŸ“ˆ Training Loss

Training loss decreased consistently during fine-tuning.

![Training Loss](assets/training_loss.png)

---

## ğŸ§  Fine-Tuning Setup

- Base Model: LLaMA-2
- Method: QLoRA (4-bit quantization)
- Optimizer: paged_adamw_32bit
- Training Type: Instruction tuning

---

## ğŸ”¬ Inference Evolution

### Step 1 â€” Model Verification (Repetition Issue)

![Inference Step 1](assets/inference_v1.png)

---

### Step 2 â€” Fix Prompt Echoing & Infinite Loops

![Inference Step 2](assets/inference_v2.png)

---

### Step 3 â€” Fix Dataset Formatting Tokens

![Inference Step 3](assets/inference_v3.png)

---

## â–¶ï¸ How to Run

### 1ï¸âƒ£ Clone the repository
text
```
git clone https://github.com/btboilerplate/Llama-2.git  
cd Llama-2
```

---

### 2ï¸âƒ£ Install dependencies
text
```
pip install -r requirements.txt
```

---

### 3ï¸âƒ£ Dataset Preparation (Optional â€“ Large Dataset Formatting)

Run this step only if you want to recreate the formatted dataset from:

timdettmers/openassistant-guanaco

Open and execute:
text
```
dataset.ipynb
```
This notebook:
- Converts conversations into LLaMA-2 instruction format
- Generates training-ready text samples

(Note: This dataset was not used for final training due to long training time.)

---

### 4ï¸âƒ£ Fine-Tuning the Model

Open and run:
text
```
llama2.ipynb
```
This notebook performs:
- Model loading with QLoRA
- Training configuration
- Fine-tuning on mlabonne/guanaco-llama2-1k dataset
- Saving adapter weights

---

### 5ï¸âƒ£ Run Inference

Inside `llama2.ipynb`, execute the inference cells sequentially to:

- Verify model generation
- Apply repetition fixes
- Remove formatting tokens
- Generate clean instruction-style responses

---

### âœ… Expected Outcome

After completion, the fine-tuned LLaMA-2 model will generate structured instruction-following responses without repetition or formatting artifacts.

---

## ğŸ§ª Key Learnings

- Dataset formatting strongly impacts instruction tuning
- QLoRA enables efficient LLM training on limited hardware
- Inference configuration is critical for stable responses
- Iterative debugging improves generation quality

---

## ğŸš€ Future Improvements

- Full OpenAssistant dataset training
- Multi-epoch fine-tuning
- Evaluation benchmarks
- Chat interface deployment

---
